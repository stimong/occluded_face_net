{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ba0420",
   "metadata": {},
   "source": [
    "# Split datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad49bc",
   "metadata": {},
   "source": [
    "## Split into training, validation, and test datasets.\n",
    "### a,b,c are split into train, val, and test in uniform proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data copy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filelist(file_dir):\n",
    "    \"\"\"\n",
    "    Get a list of files in a path.\n",
    "    Args:\n",
    "        file_dir: dataset path\n",
    "    Returns:\n",
    "        file_names: list of filenames the dataset path\n",
    "    \"\"\"\n",
    "    included_extensions = ['jpg','jpeg', 'bmp', 'png', 'gif']\n",
    "\n",
    "    file_names = [fn for fn in os.listdir(file_dir)\n",
    "                  if any(fn.lower().endswith(ext) for ext in included_extensions)]\n",
    "    print(\"Number of file_names: \", len(file_names))\n",
    "    return file_names\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    \"\"\"Creating directory\n",
    "    Args:\n",
    "        dir_name: directory name\n",
    "    \"\"\"\n",
    "    if os.path.isdir(dir_name)==False:\n",
    "        os.makedirs(dir_name)\n",
    "        print(\"create directory: \", dir_name)\n",
    "        \n",
    "def data_split(data_dict, train_rat=0.7, val_rat=0.2, test_rat=0.1):\n",
    "    \"\"\"\n",
    "    Data separation by train,val,test. Consider the data distribution.\n",
    "    Args:\n",
    "        data_dict: Full dataset dictionary.\n",
    "        train_rat: training data rate.\n",
    "        val_rat: validation data rate.\n",
    "        test_rat: test data rate. Not used but for verification.\n",
    "    Retruns:\n",
    "        sep_dict: Data dictionary separated by input rate.\n",
    "    \n",
    "    \"\"\"\n",
    "    if round(train_rat + val_rat + test_rat,2)!=1:\n",
    "        print(\"Please check the data rate. train: {}, val: {}, test: {}\".format(train_rat, val_rat, test_rat))\n",
    "\n",
    "    sep_dict = {\"train\":{}, \"val\":{}, \"test\":{}}\n",
    "    random.seed(4)\n",
    "    for k in data_dict.keys():\n",
    "        data_dict[k].sort(reverse=False)\n",
    "        random.shuffle(data_dict[k])\n",
    "\n",
    "        num_data = len(data_dict[k])\n",
    "        num_train = int(num_data*train_rat)\n",
    "        num_val = num_train+int(num_data*val_rat)\n",
    "        #num_test = num_data - num_train - num_val\n",
    "\n",
    "        sep_dict[\"train\"][k] = data_dict[k][:num_train]\n",
    "        sep_dict[\"val\"][k] = data_dict[k][num_train:num_val]\n",
    "        sep_dict[\"test\"][k] = data_dict[k][num_val:]\n",
    "        \n",
    "    print(\"separation complete!\")  \n",
    "    \n",
    "    return sep_dict\n",
    "\n",
    "def copy_org_2_new(sep_dict, org_data_dir, new_data_dir, overwrite_op = False):\n",
    "    \"\"\"\n",
    "    Copy to new dataset path.\n",
    "    Args:\n",
    "        sep_dict: Separation dataset dictionary\n",
    "        org_data_dir: Existing dataset path.\n",
    "        new_data_dir: New dataset path.\n",
    "        overwrite_op: File Overwrite Options\n",
    "    \"\"\"\n",
    "    \n",
    "    # data (train, val, test)\n",
    "    for d_type in [\"val\",\"train\",\"test\"]:\n",
    "        d_type_dir = join(new_data_dir, d_type)\n",
    "\n",
    "        # class (a0, b1, c1, bc1)\n",
    "        for c_type in sep_dict[d_type]:\n",
    "            c_type_dir = join(d_type_dir, c_type)\n",
    "            create_dir(c_type_dir) \n",
    "\n",
    "            img_path_list = sep_dict[d_type][c_type]\n",
    "            for img_path in img_path_list:\n",
    "                org_img_path = join(org_data_dir, img_path)\n",
    "                new_img_path = join(c_type_dir, img_path)\n",
    "                if overwrite_op==True:\n",
    "                    if os.path.isfile(fname)==False:\n",
    "                        shutil.copy(org_img_path, new_img_path)\n",
    "                else:\n",
    "                    shutil.copy(org_img_path, new_img_path)\n",
    "                    \n",
    "    print(\"copy complete!\")\n",
    "    \n",
    "def set_dict_occluded_face_dataset(file_names):\n",
    "    \"\"\"\n",
    "    Classify occluded_face_dataset into a,b,c values and make it into a dictionary.\n",
    "    Args:\n",
    "        file_names: image path(image file name) in dataset\n",
    "    Returns:\n",
    "        data_dict: Dictionary classified as a,b,c.\n",
    "    \"\"\"\n",
    "    # a_b_cxxxxx.extension\n",
    "    abc_dict = {0:\"a\", 2:\"b\", 4:\"c\"}\n",
    "\n",
    "    # 0: Non-occluded, 1: Occluded\n",
    "    data_dict = {\"a0\":[], \"b1\":[], \"c1\":[], \"bc1\":[]}\n",
    "    for i, file_name in enumerate(file_names):\n",
    "\n",
    "        b_sum = 0\n",
    "        c_sum = 0\n",
    "\n",
    "        for j in abc_dict.keys():\n",
    "            fnj = file_name[j] \n",
    "            if fnj.isdigit()==False:\n",
    "                break\n",
    "\n",
    "            if abc_dict[j]==\"a\":\n",
    "                if int(fnj)==0:\n",
    "                    break\n",
    "\n",
    "            elif abc_dict[j]==\"b\":\n",
    "                if int(fnj)==1:\n",
    "                    b_sum+=1\n",
    "\n",
    "            elif abc_dict[j]==\"c\":\n",
    "                if int(fnj)==1:\n",
    "                    c_sum+=1\n",
    "\n",
    "        if b_sum>0 and c_sum==0:\n",
    "            data_dict[\"b1\"].append(file_name)\n",
    "        elif b_sum==0 and c_sum>0:\n",
    "            data_dict[\"c1\"].append(file_name)\n",
    "        elif b_sum>0 and c_sum>0:\n",
    "            data_dict[\"bc1\"].append(file_name)\n",
    "        else:\n",
    "            data_dict[\"a0\"].append(file_name)\n",
    "    return data_dict\n",
    "\n",
    "def split_dataset_and_copy(data_dir, cp_data_dir, file_names, \n",
    "                           train_rat=0.7, \n",
    "                           val_rat=0.2, \n",
    "                           test_rat=0.1, \n",
    "                           overwrite_op = False):\n",
    "    \"\"\"\n",
    "    split into train, validation and testing. And copy the file to the cp_data_dir.\n",
    "    Args:\n",
    "        data_dir: dataset before copy.\n",
    "        cp_data_dir: dataset after copy.\n",
    "        file_names: List of file names before copying.\n",
    "        train_rat: training data rate.\n",
    "        val_rat: validation data rate.\n",
    "        test_rat: test data rate. Not used but for verification.\n",
    "        overwrite_op: File Overwrite Options\n",
    "    \"\"\"\n",
    "\n",
    "    num_data = len(file_names)\n",
    "    num_train = int(num_data*train_rat)\n",
    "    num_val = num_train+int(num_data*val_rat)\n",
    "\n",
    "    split_dict = {}\n",
    "    split_dict[\"train\"] = file_names[:num_train]\n",
    "    split_dict[\"val\"] = file_names[num_train:num_val]\n",
    "    split_dict[\"test\"] = file_names[num_val:]\n",
    "\n",
    "    for key_name in split_dict.keys():\n",
    "        for file_name in split_dict[key_name]:\n",
    "            org_file_path = join(data_dir,file_name)\n",
    "            \n",
    "            cp_data_dir_bg = join(cp_data_dir, key_name, \"bg\")\n",
    "            create_dir(cp_data_dir_bg)\n",
    "            \n",
    "            cp_file_path = join(cp_data_dir_bg, file_name)\n",
    "            #print(org_file_path, cp_file_path)\n",
    "            if overwrite_op==True:\n",
    "                if os.path.isfile(fname)==False:\n",
    "                    shutil.copy(org_file_path, cp_file_path)\n",
    "            else:\n",
    "                shutil.copy(org_file_path, cp_file_path)\n",
    "                    \n",
    "    print(\"copy complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb185d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfd81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "org_data_dir = \"{}/new_train_3\".format(root_dir)\n",
    "new_data_dir = \"{}/face_datasets_4\".format(root_dir)\n",
    "\n",
    "file_names = get_filelist(org_data_dir)\n",
    "\n",
    "data_dict = set_dict_occluded_face_dataset(file_names)\n",
    "sep_dict = data_split(data_dict, train_rat=0.7, val_rat=0.2, test_rat=0.1)\n",
    "copy_org_2_new(sep_dict, org_data_dir, new_data_dir, overwrite_op = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454a552",
   "metadata": {},
   "source": [
    "---\n",
    "## Adding background dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91596293",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_data_dir = \"{}/background_4\".format(root_dir)\n",
    "bg_file_names = get_filelist(bg_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c16d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_data_dir = \"{}/face_datasets_4\".format(root_dir)\n",
    "split_dataset_and_copy(bg_data_dir, cp_data_dir, bg_file_names, train_rat=0.7, val_rat=0.2, test_rat=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c7489",
   "metadata": {},
   "source": [
    "---\n",
    "## View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065f3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cv2\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import pathlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9778ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = new_data_dir\n",
    "cpt = sum([len(files) for r, d, files in os.walk(data_dir)])\n",
    "print(\"Number of data: \", cpt)\n",
    "\n",
    "data_dir2 = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5348a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show train image \n",
    "train_sample=list(data_dir2.glob('train/*/*'))\n",
    "PIL.Image.open(str(train_sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf60f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show validation image \n",
    "val_sample=list(data_dir2.glob('val/*/*'))\n",
    "PIL.Image.open(str(val_sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565cf64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show test image \n",
    "test_sample=list(data_dir2.glob('test/*/*'))\n",
    "PIL.Image.open(str(test_sample[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
